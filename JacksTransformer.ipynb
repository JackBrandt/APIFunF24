{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO0MeYom8tEQRMfdC8smmD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackBrandt/APIFunF24/blob/master/JacksTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW 11: Build a Transformer"
      ],
      "metadata": {
        "id": "V6Dy7ov7VvOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "from numpy.random import randint\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "alphabet=list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
        "alphabet_length=len(alphabet)\n",
        "char_to_idx = {char: idx for idx, char in enumerate(alphabet)}\n",
        "\n",
        "def token_vectoriztion(token):\n",
        "  if token not in alphabet:\n",
        "    return torch.zeros(len(alphabet),device=device)\n",
        "  index=alphabet.index(token)\n",
        "  vector=torch.zeros(len(alphabet),device=device)\n",
        "  vector[index]=1.0\n",
        "  return vector\n",
        "\n",
        "def token_list_vectorization(tokens):\n",
        "    idx_list = []\n",
        "    mask = []\n",
        "    for token in tokens:\n",
        "        if token in char_to_idx:\n",
        "            idx_list.append(char_to_idx[token])\n",
        "            mask.append(1)\n",
        "        else:\n",
        "            idx_list.append(0)\n",
        "            mask.append(0)\n",
        "    indices = torch.tensor(idx_list, device=device)\n",
        "    one_hot = F.one_hot(indices, num_classes=alphabet_length).float()\n",
        "    mask_tensor = torch.tensor(mask, device=device).unsqueeze(1).float()\n",
        "    return one_hot * mask_tensor\n",
        "\n",
        "def batch_token_vectorization(batch):\n",
        "    return torch.stack([token_list_vectorization(tokens) for tokens in batch])\n",
        "\n",
        "def vector_to_token(vector):\n",
        "  return alphabet[vector.argmax()]\n",
        "\n",
        "denom_vector=torch.arange(0,31)\n",
        "denom_vector=torch.pow(1000,denom_vector/2)\n",
        "\n",
        "def single_position(index):\n",
        "  vector=torch.full((int(alphabet_length/2),),float(index),device=device)\n",
        "  vector=torch.div(vector,denom_vector.to(device))\n",
        "  sin_vec=torch.sin(vector)\n",
        "  cos_vec=torch.cos(vector)\n",
        "  vector=torch.cat((sin_vec,cos_vec),0)\n",
        "  return vector\n",
        "\n",
        "def positional_embedding(token_list):\n",
        "  length=len(token_list)\n",
        "  index_vector=torch.arange(length,device=device)\n",
        "  positional_embedding=torch.stack([single_position(index) for index in index_vector])\n",
        "  return positional_embedding\n",
        "\n",
        "def positional_embedding_batch(length):\n",
        "    half_dim = alphabet_length // 2  # Using half for sine and half for cosine.\n",
        "    indices = torch.arange(0, length, device=device).unsqueeze(1).float()  # Shape: [length, 1]\n",
        "    # Compute denominators in one vectorized step.\n",
        "    denom = torch.pow(1000, torch.arange(0, half_dim, device=device).float() / 2)\n",
        "    scaled = indices / denom  # Broadcast to shape: [length, half_dim]\n",
        "    sin_embed = torch.sin(scaled)\n",
        "    cos_embed = torch.cos(scaled)\n",
        "    return torch.cat([sin_embed, cos_embed], dim=1)  # Final shape: [length, alphabet_length]\n",
        "\n",
        "def softmax(tensor):\n",
        "  softmax=torch.exp(tensor) / torch.sum(torch.exp(tensor), axis=0)\n",
        "  return softmax\n",
        "\n",
        "\n",
        "class jacks_transformer(nn.Module):\n",
        "  def __init__(self, context_size=4):\n",
        "    super(jacks_transformer, self).__init__()\n",
        "    self.context_size=context_size\n",
        "    self.alphabet=list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
        "    self.alphabet_length=len(self.alphabet)\n",
        "    self.encoder = encoder(num_layers=1, num_heads=1, alphabet_length=self.alphabet_length)\n",
        "    self.output_NN=output_NN(alphabet_length)\n",
        "\n",
        "  def get_batch(self,input_strings):\n",
        "    batch=[list(string) for string in input_strings]\n",
        "    for i,string in enumerate(batch):\n",
        "      while len(string)<self.context_size:\n",
        "        string.append(' ')\n",
        "        if len(string)<self.context_size:\n",
        "          string.insert(0,' ')\n",
        "      batch[i]=string[:self.context_size]\n",
        "    return batch\n",
        "\n",
        "  def token_vectoriztion(self,batch):\n",
        "    return batch_token_vectorization(batch)\n",
        "\n",
        "  def positional_embedding(self,batch):\n",
        "    pos_embed = positional_embedding_batch(self.context_size)\n",
        "    return pos_embed.unsqueeze(0).expand(len(batch), -1, -1)\n",
        "\n",
        "  def combined_embedding(self,input_strings):\n",
        "    batch=self.get_batch(input_strings)\n",
        "    return self.token_vectoriztion(batch)+self.positional_embedding(batch)\n",
        "\n",
        "  def forward(self,input):\n",
        "    embedding=self.combined_embedding(input)\n",
        "    encoding=self.encoder(embedding)\n",
        "    aggregated = torch.mean(encoding, dim=1)\n",
        "    output=self.output_NN(aggregated)\n",
        "    return output\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, alphabet_length, num_heads):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.attention_heads = nn.ModuleList([attention_head(alphabet_length) for _ in range(num_heads)])\n",
        "        self.proj_weights = nn.Parameter(torch.rand((num_heads * alphabet_length, alphabet_length)))\n",
        "        self.layer_norm1 = nn.LayerNorm(alphabet_length)\n",
        "        self.ffnn = FFNN(alphabet_length)\n",
        "        self.layer_norm2 = nn.LayerNorm(alphabet_length)\n",
        "\n",
        "    def forward(self, x):\n",
        "        head_outputs = [head(x) for head in self.attention_heads]\n",
        "        concatenated = torch.cat(head_outputs, dim=2)\n",
        "        projected = torch.matmul(concatenated, self.proj_weights)\n",
        "        attention_out = self.layer_norm1(x + projected)\n",
        "        ffnn_out = self.ffnn(attention_out)\n",
        "        out = self.layer_norm2(attention_out + ffnn_out)\n",
        "        return out\n",
        "\n",
        "class encoder(nn.Module):\n",
        "    def __init__(self, num_layers, num_heads, alphabet_length):\n",
        "        super(encoder, self).__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(alphabet_length, num_heads) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class attention_head(nn.Module):\n",
        "  def __init__(self,alphabet_length):\n",
        "    super(attention_head, self).__init__()\n",
        "    self.query_weights=nn.Parameter(torch.rand((alphabet_length,alphabet_length)))\n",
        "    self.key_weights=nn.Parameter(torch.rand((alphabet_length,alphabet_length)))\n",
        "    self.value_weights=nn.Parameter(torch.rand((alphabet_length,alphabet_length)))\n",
        "\n",
        "  def forward(self,batch,hide=True):\n",
        "    query=torch.matmul(batch,self.query_weights)\n",
        "    key=torch.matmul(batch,self.key_weights)\n",
        "    value=torch.matmul(batch,self.value_weights)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
        "    scale_factor = math.sqrt(query.size(-1))\n",
        "    scores = scores / scale_factor\n",
        "    attention_weights = torch.softmax(scores, dim=-1)\n",
        "    return torch.bmm(attention_weights, value)\n",
        "\n",
        "\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, alphabet_length, hidden_dim=None, num_layers=2):\n",
        "        super(FFNN, self).__init__()\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = alphabet_length\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(alphabet_length, hidden_dim))\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(num_layers - 2):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(hidden_dim, alphabet_length))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class output_NN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super(output_NN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class alphabet_generator():\n",
        "  def __init__(self):\n",
        "    self.alphabet=list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
        "    self.alphabet_length=len(self.alphabet)\n",
        "  def make_L_string(self):\n",
        "    n=1\n",
        "    #n=randint(1,5)\n",
        "    i=randint(0,len(self.alphabet))\n",
        "    j=randint(0,len(self.alphabet))\n",
        "    while j==i:\n",
        "      j=randint(0,len(self.alphabet))\n",
        "    return self.alphabet[i]*n+self.alphabet[j]*n+self.alphabet[i]*n\n",
        "\n",
        "  def make_almostL_string(self):\n",
        "    n=1\n",
        "    #n=randint(1,5)\n",
        "    i=randint(0,len(self.alphabet))\n",
        "    j=randint(0,len(self.alphabet))\n",
        "    while j==i:\n",
        "      j=randint(0,len(self.alphabet))\n",
        "    x=randint(0,6)\n",
        "    # Add error\n",
        "    match x:\n",
        "      case 0:\n",
        "        return self.alphabet[i]*(n+1)+self.alphabet[j]*n+self.alphabet[i]*n\n",
        "      case 1:\n",
        "        return self.alphabet[i]*n+self.alphabet[j]*(n+1)+self.alphabet[i]*n\n",
        "      case 2:\n",
        "        return self.alphabet[i]*n+self.alphabet[j]*n+self.alphabet[i]*(n+1)\n",
        "      case 3:\n",
        "        return self.alphabet[i]*(n-1)+self.alphabet[j]*n+self.alphabet[i]*n\n",
        "      case 4:\n",
        "        return self.alphabet[i]*n+self.alphabet[j]*(n-1)+self.alphabet[i]*n\n",
        "      case 5:\n",
        "        return self.alphabet[i]*n+self.alphabet[j]*n+self.alphabet[i]*(n-1)\n",
        "\n",
        "  def make_random_language_data(self,volume):\n",
        "    train_strings=[]\n",
        "    train_labels=[]\n",
        "    for _ in range(volume):\n",
        "      if random.randint(0,1)==1:\n",
        "        train_strings.append(self.make_L_string())\n",
        "        train_labels.append([1.0])\n",
        "      else:\n",
        "        train_strings.append(self.make_almostL_string())\n",
        "        train_labels.append([0.0])\n",
        "    return train_strings,train_labels\n",
        "\n",
        "  def make_language_data(self,volume):\n",
        "    return [self.make_L_string() for _ in range(volume)], [1 for _ in range(volume)]\n",
        "\n",
        "  def make_not_language_data(self,volume):\n",
        "    return [self.make_almostL_string() for _ in range(volume)], [0 for _ in range(volume)]\n",
        "\n",
        "\n",
        "def train(model, optimizer, loss_fn, train_loader, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for batch in train_loader:\n",
        "            batch_strings, batch_labels = batch  # batch_strings is a list of strings, batch_labels is a tensor or list of floats.\n",
        "            # Convert batch_labels to a tensor and move to device.\n",
        "            batch_labels_tensor = torch.tensor(batch_labels, dtype=torch.float32, device=device).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # Pass the raw strings in the batch to your model; your forward() will compute the embedding etc.\n",
        "            predictions = model(batch_strings)  # shape: [batch_size, 1]\n",
        "            loss = loss_fn(predictions, batch_labels_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, test_strings, test_labels, threshold=0.5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      logits = model(test_strings)\n",
        "      predictions = torch.sigmoid(logits)  # shape: [batch_size, 1]\n",
        "    # Convert predictions to binary outputs based on the threshold.\n",
        "    pred_labels = (predictions >= threshold).float()\n",
        "    test_labels_tensor = torch.tensor(test_labels, dtype=torch.float32)\n",
        "    i=0\n",
        "    tally=0\n",
        "    for string, true_label, pred, prob in zip(test_strings, test_labels_tensor, pred_labels, predictions):\n",
        "      i+=1\n",
        "      print(f\"{i}: String: {string}, True: {int(true_label.item())}, Predicted: {int(pred.item())}, Probability: {prob.item():.4f}\")\n",
        "      if int(true_label.item())==int(pred.item()):\n",
        "        tally+=1\n",
        "    print(f\"Accuracy: {tally/len(test_strings)}\")\n",
        "\n",
        "class LanguageDataset(Dataset):\n",
        "    def __init__(self, strings, labels):\n",
        "        \"\"\"\n",
        "        strings: list of raw string examples\n",
        "        labels: list (or list of lists) of labels (e.g., [1.0] or [0.0])\n",
        "        \"\"\"\n",
        "        self.strings = strings\n",
        "        # Ensure labels are simple floats\n",
        "        self.labels = [float(l[0]) if isinstance(l, list) else float(l) for l in labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.strings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a tuple: (string, label)\n",
        "        return self.strings[idx], self.labels[idx]\n",
        "\n",
        "AG = alphabet_generator()\n",
        "JT = jacks_transformer().to(device)\n",
        "optimizer = optim.Adam(JT.parameters(), lr=0.002)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "training_data=AG.make_random_language_data(800)\n",
        "train_strings=training_data[0]\n",
        "train_labels=training_data[1]\n",
        "\n",
        "# Assume train_strings and train_labels were generated by your generator.\n",
        "train_dataset = LanguageDataset(train_strings, train_labels)\n",
        "# Change the batch_size as needed; here we use 32.\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "print(\"Training...\")\n",
        "train(JT, optimizer, loss_fn, train_loader, num_epochs=50)\n",
        "\n",
        "example_strings=AG.make_random_language_data(32)\n",
        "print()\n",
        "print('Transformer Output:')\n",
        "\n",
        "print()\n",
        "print('Transformer Evaluation:')\n",
        "evaluate(JT, example_strings[0], example_strings[1])"
      ],
      "metadata": {
        "id": "KJF9L2TZijYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86ece1e1-f96c-4f80-e4da-293d0ffb4339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-fcc65eb4ea7e>:254: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_labels_tensor = torch.tensor(batch_labels, dtype=torch.float32, device=device).unsqueeze(1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Loss: 0.6945\n",
            "Epoch 20/50, Loss: 0.5384\n",
            "Epoch 30/50, Loss: 0.4886\n",
            "Epoch 40/50, Loss: 0.4672\n",
            "Epoch 50/50, Loss: 0.4647\n",
            "\n",
            "Transformer Output:\n",
            "\n",
            "Transformer Evaluation:\n",
            "1: String: qCq, True: 1, Predicted: 1, Probability: 0.9320\n",
            "2: String: HxHH, True: 0, Predicted: 1, Probability: 0.9190\n",
            "3: String: FeF, True: 1, Predicted: 1, Probability: 0.9443\n",
            "4: String: MPMM, True: 0, Predicted: 0, Probability: 0.3952\n",
            "5: String: fmf, True: 1, Predicted: 1, Probability: 0.6544\n",
            "6: String: p3pp, True: 0, Predicted: 1, Probability: 0.8718\n",
            "7: String: iXi, True: 1, Predicted: 1, Probability: 0.9352\n",
            "8: String: ZM, True: 0, Predicted: 0, Probability: 0.3805\n",
            "9: String: z5z, True: 1, Predicted: 0, Probability: 0.4338\n",
            "10: String: UVU, True: 1, Predicted: 1, Probability: 0.9187\n",
            "11: String: eSe, True: 1, Predicted: 1, Probability: 0.8885\n",
            "12: String: uP, True: 0, Predicted: 0, Probability: 0.4183\n",
            "13: String: cTcc, True: 0, Predicted: 0, Probability: 0.3811\n",
            "14: String: Mz, True: 0, Predicted: 0, Probability: 0.3250\n",
            "15: String: 0k, True: 0, Predicted: 0, Probability: 0.4548\n",
            "16: String: II, True: 0, Predicted: 0, Probability: 0.3891\n",
            "17: String: og, True: 0, Predicted: 0, Probability: 0.1737\n",
            "18: String: lXl, True: 1, Predicted: 1, Probability: 0.9321\n",
            "19: String: mJm, True: 1, Predicted: 0, Probability: 0.3737\n",
            "20: String: mIm, True: 1, Predicted: 1, Probability: 0.9377\n",
            "21: String: W4, True: 0, Predicted: 0, Probability: 0.3298\n",
            "22: String: mom, True: 1, Predicted: 0, Probability: 0.1737\n",
            "23: String: NNCN, True: 0, Predicted: 0, Probability: 0.3430\n",
            "24: String: GFG, True: 1, Predicted: 1, Probability: 0.8471\n",
            "25: String: tzt, True: 1, Predicted: 1, Probability: 0.9350\n",
            "26: String: 8y88, True: 0, Predicted: 1, Probability: 0.9228\n",
            "27: String: 2x2, True: 1, Predicted: 0, Probability: 0.4428\n",
            "28: String: 9U9, True: 1, Predicted: 0, Probability: 0.3963\n",
            "29: String: FI, True: 0, Predicted: 1, Probability: 0.8478\n",
            "30: String: cgc, True: 1, Predicted: 1, Probability: 0.7660\n",
            "31: String: nw, True: 0, Predicted: 1, Probability: 0.5752\n",
            "32: String: SgS, True: 1, Predicted: 1, Probability: 0.7660\n",
            "Accuracy: 0.6875\n"
          ]
        }
      ]
    }
  ]
}